{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py as h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_dataset = h5py.File('signs_dataset/train_signs.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('signs_dataset/test_signs.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_filters_on_grid(w_filter):\n",
    "    '''\n",
    "    Visualize conv. filters as an image (mostly for the 1st layer).\n",
    "    Arranges filters into a grid, with some paddings between adjacent filters.\n",
    "    \n",
    "    Args:\n",
    "        kernel: tensor of shape [Y, X, NumChannels, NumKernels]\n",
    "        pad: number of black pixels around each filter (between them)\n",
    "    \n",
    "    Return:\n",
    "        Tensor of shape.\n",
    "    '''\n",
    "    pad = 1\n",
    "    padding = tf.constant([[pad, pad], [pad, pad],[0,0],[0,0]]) #padding of 1 around first 2d filter of n*n\n",
    "    padded_filt = tf.pad(w_filter,padding,\"CONSTANT\")\n",
    "    \n",
    "    padded_filt = tf.transpose(padded_filt) #this is done so that filter of n*n goes to last dimension\n",
    "    \n",
    "    filt_size = padded_filt.get_shape()[3]\n",
    "    num_chan = padded_filt.get_shape()[0]\n",
    "    num_outfilt = padded_filt.get_shape()[1]\n",
    "    \n",
    "    filt_size = tf.cast(filt_size, tf.int32)   #typecasting and stacking because issue with tf.reshape command\n",
    "    num_chan = tf.cast(num_chan,tf.int32)\n",
    "    num_outfilt = tf.cast(num_outfilt,tf.int32)\n",
    "    roll_shape = tf.stack([1,num_chan*filt_size,num_outfilt*filt_size,1])\n",
    "    \n",
    "    grid_x = filt_size * filt_size * num_chan\n",
    "    grid_y = num_outfilt\n",
    "    \n",
    "    #grid = tf.cast(grid_x * grid_y,tf.int32)\n",
    "    \n",
    "    grid = tf.reshape(padded_filt,roll_shape)\n",
    "    #print(grid.get_shape())\n",
    "        \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Training samples (batch): 1080\n",
      "No. of Test samples: 120\n",
      "No. of gestures: 6\n",
      "Image format (Len, Wid, Chan): 64 * 64 * 3\n"
     ]
    }
   ],
   "source": [
    "LOGDIR = \"/home/vishnu/Dropbox/intel_works/ipython_notebooks/tf_tests/log_alexnet\"\n",
    "\n",
    "#loading the dataset\n",
    "train_x, train_y, test_x, test_y, classes = load_dataset()\n",
    "#index = 1079\n",
    "#plt.imshow(train_set_x_orig[index])\n",
    "#print(test_x.shape)\n",
    "print(\"No. of Training samples (batch): %d\"%train_x.shape[0])\n",
    "print(\"No. of Test samples: %d\"%test_x.shape[0])\n",
    "print(\"No. of gestures: %d\"%classes.size)\n",
    "print(\"Image format (Len, Wid, Chan): %d * %d * %d\"%(train_x.shape[1],train_x.shape[2],train_x.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, train_x.shape[1],train_x.shape[2],train_x.shape[3]])\n",
    "y = tf.placeholder(tf.float32, [None, classes.size])\n",
    "\n",
    "x = tf.cast(x,tf.float32) # casting as the original was uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 55, 55, 96)\n"
     ]
    }
   ],
   "source": [
    "# Non-Alexnet conv layer to make it to alexnet spec\n",
    "with tf.name_scope('nonalex1_conv1'):\n",
    "    ch_X = tf.cast(x.shape[3],tf.int32)\n",
    "    l1_filter_size = 10 # filter size of n*n for convolution\n",
    "    l1_filter_Num = 96 # No. of filters required\n",
    "    stride_1 = [1,1,1,1]\n",
    "    \n",
    "    w = tf.Variable(tf.truncated_normal([l1_filter_size, l1_filter_size, ch_X, l1_filter_Num], stddev=0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[l1_filter_Num]), name=\"B\")\n",
    "    \n",
    "    z_conv = tf.nn.conv2d(x,w,stride_1,padding=\"VALID\") + b\n",
    "    nonalex1_act1 = tf.nn.relu(z_conv,name=\"act1\")\n",
    "\n",
    "    tf.summary.histogram(\"weights\", w)\n",
    "    tf.summary.histogram(\"biases\", b)\n",
    "    tf.summary.histogram(\"activations\", nonalex1_act1)\n",
    "    print(nonalex1_act1.get_shape())\n",
    "    \n",
    "    #grid = put_filters_on_grid(w)\n",
    "    #print(grid.get_shape())\n",
    "    #tf.summary.image('nonalex1_conv1_kernal', grid, max_outputs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 27, 27, 96)\n"
     ]
    }
   ],
   "source": [
    "#max pooling layer 1\n",
    "with tf.name_scope('max_pool1'):\n",
    "    ch_2 = tf.cast(nonalex1_act1.shape[3],tf.int32)\n",
    "    k_size = [1,3,3,1] #filter size\n",
    "    stride_2 = [1,2,2,1]\n",
    "    act2_maxpool1 = tf.nn.avg_pool(nonalex1_act1,k_size,stride_2,padding='VALID',name=\"act2_maxpool1\")\n",
    "    print(act2_maxpool1.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 27, 27, 256)\n"
     ]
    }
   ],
   "source": [
    "# Alexnet conv layer 1\n",
    "with tf.name_scope('alex1_conv2'):\n",
    "    ch_3 = tf.cast(act2_maxpool1.shape[3],tf.int32)\n",
    "    l3_filter_size = 5 # filter size of n*n for convolution\n",
    "    l3_filter_Num = 256 # No. of filters required\n",
    "    stride_3 = [1,1,1,1]\n",
    "    \n",
    "    w = tf.Variable(tf.truncated_normal([l3_filter_size, l3_filter_size, ch_3, l3_filter_Num], stddev=0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[l3_filter_Num]), name=\"B\")\n",
    "    \n",
    "    z_conv = tf.nn.conv2d(act2_maxpool1,w,stride_3,padding=\"SAME\") + b\n",
    "    alex1_act3 = tf.nn.relu(z_conv,name=\"alex1_act3\")\n",
    "\n",
    "    tf.summary.histogram(\"weights\", w)\n",
    "    tf.summary.histogram(\"biases\", b)\n",
    "    tf.summary.histogram(\"activations\", alex1_act3)\n",
    "    print(alex1_act3.get_shape())\n",
    "    \n",
    "    #grid = put_filters_on_grid(w)\n",
    "    #print(grid.get_shape())\n",
    "    #tf.summary.image('alex1_act3_kernal', grid, max_outputs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 13, 13, 256)\n"
     ]
    }
   ],
   "source": [
    "#max pooling layer 2\n",
    "with tf.name_scope('max_pool2'):\n",
    "    ch_4 = tf.cast(alex1_act3.shape[3],tf.int32)\n",
    "    k_size = [1,3,3,1] #filter size\n",
    "    stride_4 = [1,2,2,1]\n",
    "    act4_maxpool2 = tf.nn.avg_pool(alex1_act3,k_size,stride_4,padding='VALID',name=\"act4_maxpool2\")\n",
    "    print(act4_maxpool2.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 13, 13, 384)\n"
     ]
    }
   ],
   "source": [
    "# Alexnet conv layer 2\n",
    "with tf.name_scope('alex2_conv3'):\n",
    "    ch_5 = tf.cast(act4_maxpool2.shape[3],tf.int32)\n",
    "    l5_filter_size = 3 # filter size of n*n for convolution\n",
    "    l5_filter_Num = 384 # No. of filters required\n",
    "    stride_5 = [1,1,1,1]\n",
    "    \n",
    "    w = tf.Variable(tf.truncated_normal([l5_filter_size, l5_filter_size, ch_5, l5_filter_Num], stddev=0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[l5_filter_Num]), name=\"B\")\n",
    "    \n",
    "    z_conv = tf.nn.conv2d(act4_maxpool2,w,stride_5,padding=\"SAME\") + b\n",
    "    alex2_act5 = tf.nn.relu(z_conv,name=\"alex2_act5\")\n",
    "\n",
    "    tf.summary.histogram(\"weights\", w)\n",
    "    tf.summary.histogram(\"biases\", b)\n",
    "    tf.summary.histogram(\"activations\", alex2_act5)\n",
    "    print(alex2_act5.get_shape())\n",
    "    \n",
    "    #grid = put_filters_on_grid(w)\n",
    "    #print(grid.get_shape())\n",
    "    #tf.summary.image('alex2_act5', grid, max_outputs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 13, 13, 384)\n"
     ]
    }
   ],
   "source": [
    "# Alexnet conv layer 3\n",
    "with tf.name_scope('alex3_conv4'):\n",
    "    ch_6 = tf.cast(alex2_act5.shape[3],tf.int32)\n",
    "    l6_filter_size = 3 # filter size of n*n for convolution\n",
    "    l6_filter_Num = 384 # No. of filters required\n",
    "    stride_6 = [1,1,1,1]\n",
    "    \n",
    "    w = tf.Variable(tf.truncated_normal([l6_filter_size, l6_filter_size, ch_6, l6_filter_Num], stddev=0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[l6_filter_Num]), name=\"B\")\n",
    "    \n",
    "    z_conv = tf.nn.conv2d(alex2_act5,w,stride_6,padding=\"SAME\") + b\n",
    "    alex3_act6 = tf.nn.relu(z_conv,name=\"alex3_act6\")\n",
    "\n",
    "    tf.summary.histogram(\"weights\", w)\n",
    "    tf.summary.histogram(\"biases\", b)\n",
    "    tf.summary.histogram(\"activations\", alex3_act6)\n",
    "    print(alex3_act6.get_shape())\n",
    "    \n",
    "    #grid = put_filters_on_grid(w)\n",
    "    #print(grid.get_shape())\n",
    "    #tf.summary.image('alex3_act6', grid, max_outputs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 13, 13, 256)\n"
     ]
    }
   ],
   "source": [
    "# Alexnet conv layer 4\n",
    "with tf.name_scope('alex4_conv5'):\n",
    "    ch_7 = tf.cast(alex3_act6.shape[3],tf.int32)\n",
    "    l7_filter_size = 3 # filter size of n*n for convolution\n",
    "    l7_filter_Num = 256 # No. of filters required\n",
    "    stride_7 = [1,1,1,1]\n",
    "    \n",
    "    w = tf.Variable(tf.truncated_normal([l7_filter_size, l7_filter_size, ch_7, l7_filter_Num], stddev=0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[l7_filter_Num]), name=\"B\")\n",
    "    \n",
    "    z_conv = tf.nn.conv2d(alex3_act6,w,stride_7,padding=\"SAME\") + b\n",
    "    alex4_act7 = tf.nn.relu(z_conv,name=\"alex4_act7\")\n",
    "\n",
    "    tf.summary.histogram(\"weights\", w)\n",
    "    tf.summary.histogram(\"biases\", b)\n",
    "    tf.summary.histogram(\"activations\", alex4_act7)\n",
    "    print(alex4_act7.get_shape())\n",
    "    \n",
    "    #grid = put_filters_on_grid(w)\n",
    "    #print(grid.get_shape())\n",
    "    #tf.summary.image('alex4_act7', grid, max_outputs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 6, 6, 256)\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#max pooling layer 3\n",
    "with tf.name_scope('max_pool3'):\n",
    "    ch_8 = tf.cast(alex4_act7.shape[3],tf.int32)\n",
    "    k_size = [1,3,3,1] #filter size\n",
    "    stride_8 = [1,2,2,1]\n",
    "    act8_maxpool3 = tf.nn.avg_pool(alex4_act7,k_size,stride_8,padding='VALID',name=\"act8_maxpool3\")\n",
    "    print(act8_maxpool3.get_shape())\n",
    "    print(act8_maxpool3.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 9216)\n"
     ]
    }
   ],
   "source": [
    "# flattining\n",
    "act8_flat = tf.reshape(act8_maxpool3, [-1, 6 * 6 * 256])\n",
    "print(act8_flat.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4096)\n"
     ]
    }
   ],
   "source": [
    "# %% Create a fully-connected layer 1:\n",
    "with tf.name_scope('fc1') as scope:\n",
    "    #lx = tf.cast(act8_flat.get_shape()[1],tf.int32)\n",
    "    #ly = tf.cast(act8_flat.get_shape()[2],tf.int32)\n",
    "    #lz = tf.cast(act8_flat.get_shape()[3],tf.int32)\n",
    "    n_fc1 = 4096\n",
    "    W_fc1 = tf.Variable(tf.truncated_normal([6 * 6 * 256, n_fc1], stddev=0.1),name=\"w_fc1\")\n",
    "    b_fc1 = tf.Variable(tf.truncated_normal([n_fc1], stddev=0.1),name=\"b_fc1\")\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(act8_flat, W_fc1) + b_fc1)\n",
    "    \n",
    "    tf.summary.histogram(\"weights\", W_fc1)\n",
    "    tf.summary.histogram(\"biases\", b_fc1)\n",
    "    tf.summary.histogram(\"activations\", h_fc1)\n",
    "    print(h_fc1.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4096)\n"
     ]
    }
   ],
   "source": [
    "# %% Create a fully-connected layer 2:\n",
    "with tf.name_scope('fc2') as scope:\n",
    "    #lx = tf.cast(act8_flat.get_shape()[1],tf.int32)\n",
    "    #ly = tf.cast(act8_flat.get_shape()[2],tf.int32)\n",
    "    #lz = tf.cast(act8_flat.get_shape()[3],tf.int32)\n",
    "    n_fc2 = 4096\n",
    "    W_fc2 = tf.Variable(tf.truncated_normal([n_fc1, n_fc2], stddev=0.1),name=\"w_fc2\")\n",
    "    b_fc2 = tf.Variable(tf.truncated_normal([n_fc2], stddev=0.1),name=\"b_fc2\")\n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "    \n",
    "    tf.summary.histogram(\"weights\", W_fc2)\n",
    "    tf.summary.histogram(\"biases\", b_fc2)\n",
    "    tf.summary.histogram(\"activations\", h_fc2)\n",
    "    print(h_fc2.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 6)\n"
     ]
    }
   ],
   "source": [
    "# %% And finally our softmax layer:\n",
    "with tf.name_scope('softmax_layer') as scope:\n",
    "    W_fc3 = tf.Variable(tf.truncated_normal([n_fc2, 6], stddev=0.1),name=\"w_fc3\")\n",
    "    b_fc3 = tf.Variable(tf.truncated_normal([6], stddev=0.1),name=\"b_fc3\")\n",
    "    y_pred = tf.nn.softmax(tf.matmul(h_fc2, W_fc3) + b_fc3)\n",
    "    print(y_pred.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Define loss/eval/training functions\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    cross_entropy = -tf.reduce_sum(y * tf.log(y_pred))\n",
    "    \n",
    "    tf.summary.scalar(\"cross_entropy\", cross_entropy)\n",
    "\n",
    "with tf.name_scope('train'):    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?,)\n"
     ]
    }
   ],
   "source": [
    "# %% Monitor accuracy\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    print(correct_prediction.get_shape())\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ = tf.summary.merge_all()\n",
    "\n",
    "# %% We now create a new session to actually perform the initialization the\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "writer = tf.summary.FileWriter(LOGDIR)\n",
    "writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_org = np.eye(6)[train_y] #converting it into one-hot encoding\n",
    "y_org = np.reshape(y_org,[y_org.shape[1],y_org.shape[2]])\n",
    "\n",
    "y_val = np.eye(6)[test_y] #converting it into one-hot encoding\n",
    "y_val = np.reshape(y_val,[y_val.shape[1],y_val.shape[2]])\n",
    "\n",
    "ep = 10\n",
    "batch_size = 100\n",
    "\n",
    "for i in range(ep):\n",
    "    for batch_i in range(train_x.shape[0] // batch_size):\n",
    "        if i%5 == 0:\n",
    "            batch_xs, batch_ys = train_x[(batch_i+batch_i*i):batch_size,:,:,:], y_org[(batch_i+batch_i*i):batch_size,:]\n",
    "            train_accuracy = sess.run(accuracy, feed_dict={x:batch_xs, y:batch_ys})\n",
    "            print(train_accuracy)\n",
    "            #s = sess.run(summ, feed_dict={x:batch_xs, y:batch_ys})\n",
    "            #writer.add_summary(s, i)\n",
    "            \n",
    "        if i % 500 == 0:\n",
    "            #sess.run(assignment, feed_dict={x: train_x[1:100,:,:,:], y: y_org[1:100,:]})\n",
    "            #saver.save(sess, os.path.join(LOGDIR, \"model.ckpt\"), i)\n",
    "            sess.run(optimizer, feed_dict={x: train_x[1:200,:,:,:], y: y_org[1:200,:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
