
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import h5py as h5py

LOGDIR = "/home/vishnu/Dropbox/intel_works/nn_files/tf_tests/modular_py/log_com_gesture_cnn"



########################################################################################
def load_dataset():
    train_dataset = h5py.File('signs_dataset/train_signs.h5', "r")
    train_set_x_orig = np.array(train_dataset["train_set_x"][:]) # your train set features
    train_set_y_orig = np.array(train_dataset["train_set_y"][:]) # your train set labels

    test_dataset = h5py.File('signs_dataset/test_signs.h5', "r")
    test_set_x_orig = np.array(test_dataset["test_set_x"][:]) # your test set features
    test_set_y_orig = np.array(test_dataset["test_set_y"][:]) # your test set labels

    classes = np.array(test_dataset["list_classes"][:]) # the list of classes
    
    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))
    
    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes

########################################################################################



########################################################################################
def committee1_gesture_cnn(num_cnn_layers,alpha,keeprobs):
    
    #tf.reset_default_graph()
    sess = tf.Session()
    
#input placeholders
    x = tf.placeholder(tf.float32, [None, train_x.shape[1],train_x.shape[2],train_x.shape[3]])
    y = tf.placeholder(tf.float32, [None, classes.size])
    #x = tf.cast(x,tf.float32) # casting as the original was uint8

    # param dicts
    W_params = {}
    B_params = {}
    Z_params = {}
    A_params = {}
    Maxpool_params = {}

    A_params["a0"] = x
    """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

# CNN layers
    for iter_i in range(1,num_cnn_layers+1,1):
        "('string_%d'%iter_1) is the syntax to make it dynamic"
        # convolution
        with tf.name_scope('convolution'):
            W_params["w{0}".format(iter_i)] = tf.Variable( tf.truncated_normal([ Fsize_params["fsize{0}".format(iter_i)], Fsize_params["fsize{0}".format(iter_i)], ChanNum_params["chan{0}".format(iter_i)], Fnum_params["fnum{0}".format(iter_i)] ], stddev=0.1) )

            B_params["b{0}".format(iter_i)] = tf.Variable( tf.constant(0.1, shape=[Fnum_params["fnum{0}".format(iter_i)]]) )

            Z_params["z{0}".format(iter_i)] = tf.nn.conv2d( A_params["a{0}".format(iter_i-1)], W_params["w{0}".format(iter_i)], Stride_params["stride{0}".format(iter_i)], padding="VALID" ) + B_params["b{0}".format(iter_i)]

        # maxpool
        if (Maxpool_ops["pool{0}".format(iter_i)]) == True:
            with tf.name_scope('max_pool'):
                Maxpool_params["max_pool{0}".format(iter_i)] = tf.nn.max_pool( Z_params["z{0}".format(iter_i)], Pool_Ksize_params["ksize{0}".format(iter_i)], Pool_stride_params["kstride{0}".format(iter_i)], padding='VALID' )


        # relu
        with tf.name_scope('a_conv--relu'):
            if (Maxpool_ops["pool{0}".format(iter_i)]) == True:
                A_params["a{0}".format(iter_i)] = tf.nn.relu( Maxpool_params["max_pool{0}".format(iter_i)], name=('a_conv%d'%iter_i) )
            else:
                A_params["a{0}".format(iter_i)] = tf.nn.relu( Z_params["z{0}".format(iter_i)], name=('a_conv%d'%iter_i) )

        
        # for Z and not after max pool and relu
        tf.summary.histogram("weights", W_params["w{0}".format(iter_i)])
        tf.summary.histogram("biases", B_params["b{0}".format(iter_i)])

        tf.summary.histogram("activations", A_params["a{0}".format(iter_i)])
        
        print("\nshape after z_conv%d : "%iter_i)
        print( Z_params["z{0}".format(iter_i)].shape ) 
        print("\n")
        
        # after pooling & relu
        print("\nshape after pool & relu a_conv%d : "%iter_i)
        print( A_params["a{0}".format(iter_i)].shape ) 
        print("\n")
               
    
    """"""""""""""""""""""""""""""
    
# Fully connected layers
    
    #flattening for fc 
    with tf.name_scope('flattening'):
        fc_flat = tf.reshape( A_params["a{0}".format(num_cnn_layers)], [-1, (tf.shape(A_params["a{0}".format(num_cnn_layers)])[1]) * (tf.shape(A_params["a{0}".format(num_cnn_layers)])[2]) * (tf.shape(A_params["a{0}".format(num_cnn_layers)])[3])] )

    """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

    #fully-connected layer 1:
    with tf.name_scope('fc1') as scope:
        W_fc1 = tf.Variable(tf.truncated_normal([fc_shape_last_cnn, n_fc1], stddev=0.1),name="w_fc1")
        b_fc1 = tf.Variable(tf.truncated_normal([n_fc1], stddev=0.1),name="b_fc1")
        h_fc1 = tf.nn.dropout(tf.nn.relu(tf.matmul(fc_flat, W_fc1) + b_fc1), keeprobs )
        
        tf.summary.histogram("weights", W_fc1)
        tf.summary.histogram("biases", b_fc1)
        tf.summary.histogram("activations", h_fc1)
        print("\nshape of fc1: ")
        print(h_fc1.shape)
        print("\n")
    
    """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
       
    #fully-connected layer 2:
    with tf.name_scope('fc2') as scope:
        W_fc2 = tf.Variable(tf.truncated_normal([n_fc1, n_fc2], stddev=0.1),name="w_fc2")
        b_fc2 = tf.Variable(tf.truncated_normal([n_fc2], stddev=0.1),name="b_fc2")
        h_fc2 = tf.nn.dropout(tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2), keeprobs )
        
        tf.summary.histogram("weights", W_fc2)
        tf.summary.histogram("biases", b_fc2)
        tf.summary.histogram("activations", h_fc2)
        print("\nshape of fc2: ")
        print(h_fc2.shape)
        print("\n")
    
    """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
        
    #fully-connected layer 3:
    with tf.name_scope('fc3') as scope:
        W_fc3 = tf.Variable(tf.truncated_normal([n_fc2, n_fc3], stddev=0.1),name="w_fc3")
        b_fc3 = tf.Variable(tf.truncated_normal([n_fc3], stddev=0.1),name="b_fc3")
        h_fc3 = tf.nn.dropout(tf.nn.relu(tf.matmul(h_fc2, W_fc3) + b_fc3), keeprobs )
        
        tf.summary.histogram("weights", W_fc3)
        tf.summary.histogram("biases", b_fc3)
        tf.summary.histogram("activations", h_fc3)
        print("\nshape of fc3: ")
        print(h_fc3.shape)
        print("\n")
    
    """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
     
    #softmax layer:
    with tf.name_scope('final_softmax') as scope:
        W_fc4 = tf.Variable(tf.truncated_normal([n_fc3, classes.size], stddev=0.1),name="w_fc4")
        b_fc4 = tf.Variable(tf.truncated_normal([6], stddev=0.1),name="b_fc3")
        y_pred = tf.nn.softmax(tf.matmul(h_fc3, W_fc4) + b_fc4)
        #print(y_pred.get_shape())
    
    """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

    #loss functions:
    with tf.name_scope('cross_entropy'):
        cross_entropy = -tf.reduce_sum(y * tf.log(y_pred))
        
        tf.summary.scalar("cross_entropy", cross_entropy)
    
    """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

    #training:
    with tf.name_scope('train'):    
        optimizer = tf.train.AdamOptimizer(alpha).minimize(cross_entropy)
        #optimizer = tf.train.GradientDescentOptimizer(alpha).minimize(cross_entropy)
    
    """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
    
    #accuracy:
    with tf.name_scope('accuracy'):
        correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))
        
        tf.summary.scalar("accuracy", accuracy)
    
    """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

    summ = tf.summary.merge_all()
    
    sess.run(tf.global_variables_initializer())
    
    writer = tf.summary.FileWriter(LOGDIR)
    writer.add_graph(sess.graph)

    """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

    #write here batch size and runs
    ep = 200
    batch_size = 500
    
    for i in range(ep):
        for batch_i in range(train_x.shape[0] // batch_size):
            if batch_i % 1 == 0:
                batch_xs, batch_ys = train_x[(batch_i+batch_i*i):batch_size,:,:,:], y_org[(batch_i+batch_i*i):batch_size,:]
                train_accuracy = sess.run(accuracy, feed_dict={x:batch_xs, y:batch_ys})
                print(train_accuracy)
                #s = sess.run(summ, feed_dict={x:batch_xs, y:batch_ys})
                sess.run(optimizer, feed_dict={x:batch_xs, y:batch_ys})
                
                #writer.add_summary(s, i)


    print(">>>>>>>>>>>>>>>>>>>>>>>>>>> testing phase <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<")
    # test accuracy
    for num_i in  range(test_x.shape[0]):
        batch_xs, tatch_ys = test_x[num_i,:,:,:], y_val[num_i,:]
        test_accuracy = sess.run(accuracy, feed_dict={x:test_x, y:y_val})
        print(test_accuracy)

########################################################################################



""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

"""
Inputs X and Y def:
"""
train_x, train_y, test_x, test_y, classes = load_dataset()
#index = 1079
#plt.imshow(train_set_x_orig[index])
print("\n\nNo. of Training samples (batch): %d"%train_x.shape[0])
print("No. of Test samples: %d"%test_x.shape[0])
print("No. of gestures: %d"%classes.size)
print("Image format (Len, Wid, Chan): %d * %d * %d"%(train_x.shape[1],train_x.shape[2],train_x.shape[3]))

train_x = train_x/255
test_x = test_x/255

y_org = np.eye(6)[train_y] #converting it into one-hot encoding  # this is equal to y
y_org = np.reshape(y_org,[y_org.shape[1],y_org.shape[2]])

y_val = np.eye(6)[test_y] #converting it into one-hot encoding   # this is equal to y_test
y_val = np.reshape(y_val,[y_val.shape[1],y_val.shape[2]])


ChanNum_params = {}
Fsize_params = {}
Fnum_params = {}
Stride_params = {}
Maxpool_ops = {}
Pool_Ksize_params ={}
Pool_stride_params = {}

""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
### CNN layer params

num_cnn_layers = 3

#layer 1 --> conv, maxpool & relu
ChanNum_params["chan1"] = tf.cast(train_x.shape[3],tf.int32)
Fsize_params["fsize1"] = 3 # filter size of n*n for convolution
Fnum_params["fnum1"] = 6 # No. of filters required
Stride_params["stride1"] = [1,2,2,1]
Maxpool_ops["pool1"] = False #change here if pooling not required
if Maxpool_ops["pool1"] == True:
    Pool_Ksize_params["ksize1"] = [1,2,2,1]
    Pool_stride_params["kstride1"] = [1,2,2,1]
#layer 1 ends 

#layer 2 --> conv, maxpool & relu
ChanNum_params["chan2"] = Fnum_params["fnum1"]
Fsize_params["fsize2"] = 5 
Fnum_params["fnum2"] = 10 
Stride_params["stride2"] = [1,1,1,1]
Maxpool_ops["pool2"] = True
if Maxpool_ops["pool2"] == True:
    Pool_Ksize_params["ksize2"] = [1,2,2,1]
    Pool_stride_params["kstride2"] = [1,2,2,1]
#layer 2 ends 

#layer 3 --> conv, maxpool & relu
ChanNum_params["chan3"] = Fnum_params["fnum2"]
Fsize_params["fsize3"] = 5 
Fnum_params["fnum3"] = 16 
Stride_params["stride3"] = [1,1,1,1] 
Maxpool_ops["pool3"] = True
if Maxpool_ops["pool3"] == True:
    Pool_Ksize_params["ksize3"] = [1,2,2,1]
    Pool_stride_params["kstride3"] = [1,2,2,1]
#layer 3 ends 

##layer 4 --> conv, maxpool & relu
#ChanNum_params["chan4"] = Fnum_params["fnum3"]
#Fsize_params["fsize4"] = 5 
#Fnum_params["fnum4"] = 20 
#Stride_params["stride4"] = [1,1,1,1] 
#Maxpool_ops["pool4"] = True
#if Maxpool_ops["pool4"] == True:
#    Pool_Ksize_params["ksize4"] = [1,2,2,1]
#    Pool_stride_params["kstride4"] = [1,2,2,1]
##layer 4 ends 
#
##layer 5 --> conv, maxpool & relu
#ChanNum_params["chan5"] = Fnum_params["fnum4"]
#Fsize_params["fsize5"] = 5 
#Fnum_params["fnum5"] = 20 
#Stride_params["stride5"] = [1,1,1,1] 
#Maxpool_ops["pool5"] = True
#if Maxpool_ops["pool5"] == True:
#    Pool_Ksize_params["ksize5"] = [1,2,2,1]
#    Pool_stride_params["kstride5"] = [1,2,2,1]
##layer 5 ends 

### CNN layer params ends

""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
### FC layer params
num_fc_layers = 2
fc_shape_last_cnn = 256 #"this is done because tf.reshape had some issues"

#fc layer 1
n_fc1 = 128

#fc layer 2
n_fc2 = 64

#fc layer 3
n_fc3 = classes.size

### FC layer params ends

""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""


""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
#call the model here with alpha and multiple runs with for loop to change alpha
committee1_gesture_cnn(num_cnn_layers, alpha=0.0011, keeprobs=0.8)

""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
